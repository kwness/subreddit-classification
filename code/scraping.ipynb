{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cf4f6e-16f4-4de4-accd-bb5916dcd80f",
   "metadata": {},
   "source": [
    "# Scraping r/vegan and r/carnivore - Kyle Ness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c4f9d-16c3-4e95-93de-60419487f119",
   "metadata": {},
   "source": [
    "### In this file, we will be gathering reddit posts from r/vegan and r/carnivore via the pushshift api (https://github.com/pushshift/api). These posts will later be used for the fitting and evaluation of two classifiers using NLP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee81db96-2131-4cdb-b3c2-3f98b3ee34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests as req\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "94d02255-735c-496e-bbef-46e63877db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here two functions are created: a helper for pulling 100 posts at a time from a specified subreddit, and a higher level function that executes the \n",
    "#helper 10 times. End result = roughly 1,000 - 1,100 posts gathered from reddit per call.\n",
    "\n",
    "def pull_data(subreddit, before): #designed as a helper function to 'build' below\n",
    "    \n",
    "    print('Pull start') #Just using this as a status check, along with the other print statements seen below.\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    scrape_list = []\n",
    "    post_count = 0\n",
    "    \n",
    "    #Here a while loop is used because it is unknown how many valid posts (that is, ones with descriptions) will be returned per request.\n",
    "    #100 unfortunately had to be used because 1000, our target, would result in an error 'Too many requests'. Even 250 proved too much. 100 is safe.\n",
    "    while post_count < 100:\n",
    "        if req.get(url, {'subreddit': subreddit, 'size': 100, 'before': before}).status_code == 200:\n",
    "            #Pull data from specified subreddit using requests library, convert to dataframe. Keep track of min datetime so as to go further back in next search\n",
    "            posts_df = pd.DataFrame(req.get(url, {'subreddit': subreddit, 'size': 100, 'before': before}).json()['data'])\n",
    "\n",
    "            #Take dataframe and drop all entries without a post description\n",
    "            posts_w_descripts = posts_df[(posts_df['selftext'].str.len() != 0) & (posts_df['selftext'] != '[removed]') & (posts_df['selftext'].isna() == False)]\n",
    "\n",
    "            #Append this dataframe to the list of all dataframes created via scraping\n",
    "            scrape_list.append(posts_w_descripts[['subreddit', 'title', 'selftext']])\n",
    "            print(f'Scrape {len(scrape_list)}')\n",
    "\n",
    "            #Increment post_count by the number of valid entries scraped so that we know we're reaching our goal of 1,000 posts for analysis later.\n",
    "            post_count += posts_w_descripts.shape[0]\n",
    "            #Also set time to the new minimum observed in this scrape - makes sure we don't scrape the same data several times.\n",
    "            before = posts_df['created_utc'].min()\n",
    "            \n",
    "        else:\n",
    "            print('Please standby')\n",
    "            time.sleep(5)\n",
    "            \n",
    "    #return a dataframe of the 100 or so valid posts collected      \n",
    "    return (pd.concat(scrape_list, axis = 0), before)\n",
    "\n",
    "def build(subreddit): #builds a dataframe of 1,000+ scraped posts from specified subreddit\n",
    "    \n",
    "    print('START') #Status check\n",
    "    comb = []\n",
    "    before = 1656400000 #Choosing a static start for this - issues were had trying time.time(). This time is quivalent to June 28th, 3 AM EST.\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(f'Iteration {i}') #Status check\n",
    "        pull = pull_data(subreddit, before)\n",
    "        comb.append(pull[0])\n",
    "        before = pull[1]\n",
    "        time.sleep(10) #Give some exctra time in between pulls to make sure no error comes up from the API.\n",
    "        \n",
    "    return pd.concat(comb, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62c75fe9-3862-4563-b133-833cb5e86cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Iteration 0\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 1\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 2\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 3\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 4\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 5\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 6\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 7\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 8\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 9\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n"
     ]
    }
   ],
   "source": [
    "vegan_df = build('vegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c8670e1-c10a-403d-946d-642eac22c9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Iteration 0\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 1\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 2\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 3\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 4\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 5\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Scrape 5\n",
      "Iteration 6\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Iteration 7\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 8\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n",
      "Iteration 9\n",
      "Pull start\n",
      "Scrape 1\n",
      "Scrape 2\n",
      "Scrape 3\n",
      "Scrape 4\n"
     ]
    }
   ],
   "source": [
    "carn_df = build('carnivore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "013dca38-f6aa-424a-a1c3-592e8427ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1139, 3)\n",
      "(1097, 3)\n"
     ]
    }
   ],
   "source": [
    "print(vegan_df.shape)\n",
    "print(carn_df.shape) #Successful! We have over 1,000 observations for each, with some spare room if duplicates exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d31c21a2-278f-4c96-a345-a107cc80c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegan_df.drop_duplicates(inplace = True)\n",
    "carn_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba8c9c13-6f1b-4b6b-b1a5-e9c19ada5ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1128, 3)\n",
      "(1093, 3)\n"
     ]
    }
   ],
   "source": [
    "print(vegan_df.shape)\n",
    "print(carn_df.shape) #Successful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a3402ca-645d-426f-84df-8895aee609f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegan_df.drop_duplicates(subset = 'selftext', inplace = True)\n",
    "carn_df.drop_duplicates(subset = 'selftext', inplace = True) #For some reason, these did not fully work in the previous attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1691fa5b-ec15-4731-8b5e-793c69c9b97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vegan_df['selftext'].value_counts().sort_values(ascending = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c3604c3-9482-4601-8d6c-fdabf39d9f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carn_df['selftext'].value_counts().sort_values(ascending = False)[0] #Values of 1 are good --> means there are no duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34b9c1fd-21e8-47c1-ac2e-4e24fc3e21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1122, 3)\n",
      "(1088, 3)\n"
     ]
    }
   ],
   "source": [
    "print(vegan_df.shape)\n",
    "print(carn_df.shape) #Now both have 1,050 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27baf942-5592-48f1-8b21-327c2a7610a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping last x rows such that each dataframe has 1,050 observations (50 over 1,000 for some buffer in case errors come up)\n",
    "vegan_df_final = vegan_df[:1050]\n",
    "carn_df_final = carn_df[:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "802220b5-a135-44b3-a832-5fc3f4814dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 3)\n",
      "(1050, 3)\n"
     ]
    }
   ],
   "source": [
    "print(vegan_df_final.shape)\n",
    "print(carn_df_final.shape) #Now both have 1,050 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bfb23939-a61a-4483-aa47-158254bbf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, save our assembled datasets to csv so that we can use them elsewhere.\n",
    "vegan_df_final.set_index('subreddit', inplace = True)\n",
    "carn_df_final.set_index('subreddit', inplace = True)\n",
    "vegan_df_final.to_csv('../datasets/vegan.csv')\n",
    "carn_df_final.to_csv('../datasets/carnivore.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
